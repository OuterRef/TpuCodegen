#loc = loc(unknown)
#loc1 = loc("in_0")
module attributes {module.FLOPs = 33528768 : i64, module.asymmetric = true, module.chip = "bm1684x", module.coeff_addr = 4294967296 : i64, module.coeff_size = 16384 : i64, module.mode = "F32", module.name = "TorchLayerGroup_3", module.neuron_addr = 4294983680 : i64, module.neuron_size = 1605632 : i64, module.platform = "ONNX", module.state = "TPU_ADDRESSED", module.weight_file = "torchlayergroup_3_tpu_addressed_bm1684x_f32_weight.npz"} {
  func.func @main(%arg0: tensor<7x3x80x40xf32> loc(unknown)) -> (tensor<7x8x78x38xf32, 4295254016 : i64>, tensor<7x8x78x38xf32, 4295921664 : i64>) {
    %0 = "top.Input"(%arg0) : (tensor<7x3x80x40xf32>) -> tensor<7x3x80x40xf32, 4294983680 : i64> loc(#loc1)
    %1:2 = call @subfunc_0(%0) : (tensor<7x3x80x40xf32, 4294983680 : i64>) -> (tensor<7x8x78x38xf32, 4295921664 : i64>, tensor<7x8x78x38xf32, 4295254016 : i64>) loc(#loc)
    return %1#1, %1#0 : tensor<7x8x78x38xf32, 4295254016 : i64>, tensor<7x8x78x38xf32, 4295921664 : i64> loc(#loc)
  } loc(#loc)
  func.func @subfunc_0(%arg0: tensor<7x3x80x40xf32, 4294983680 : i64> loc("in_0")) -> (tensor<7x8x78x38xf32, 4295921664 : i64>, tensor<7x8x78x38xf32, 4295254016 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>} {
    %0 = "top.Weight"() : () -> tensor<1x8x3x9xf32, 4294967296 : i64> loc(#loc2)
    %1 = "top.Weight"() : () -> tensor<1x8x1x1xf32, 4294971392 : i64> loc(#loc3)
    %2 = "top.Weight"() : () -> tensor<1x8x8x9xf32, 4294975488 : i64> loc(#loc4)
    %3 = "top.Weight"() : () -> tensor<1x8x1x1xf32, 4294979584 : i64> loc(#loc5)
    %4:2 = "tpu.Group"(%arg0) ({
      %5 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 51200, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0, 4], n_slice = [4, 3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [40], id = 0, stage = 0, group_type = 0>, lmem_type = 1 : i64, use_3ic_optimize = 0 : i64} : (tensor<7x3x80x40xf32, 4294983680 : i64>) -> tensor<7x3x80x40xf32> loc(#loc8)
      %6 = "tpu.Load"(%0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 212992, out_size = 108, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [9], id = 1, stage = 0, group_type = 0>, lmem_type = 0 : i64, use_3ic_optimize = 0 : i64} : (tensor<1x8x3x9xf32, 4294967296 : i64>) -> tensor<1x8x3x9xf32> loc(#loc9)
      %7 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 245760, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, group_type = 0>, lmem_type = 0 : i64, use_3ic_optimize = 0 : i64} : (tensor<1x8x1x1xf32, 4294971392 : i64>) -> tensor<1x8x1x1xf32> loc(#loc10)
      %8 = "tpu.Conv2D"(%5, %6, %7) {coeff_merged = false, dilations = [1, 1], do_relu = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 47616, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0, 4], n_slice = [4, 3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [78], w_idx = [0], w_slice = [38], id = 3, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, with_bias = true} : (tensor<7x3x80x40xf32>, tensor<1x8x3x9xf32>, tensor<1x8x1x1xf32>) -> tensor<7x8x78x38xf32> loc(#loc6)
      %9 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 211456, out_size = 288, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [9], id = 4, stage = 1, group_type = 0>, lmem_type = 0 : i64, use_3ic_optimize = 0 : i64} : (tensor<1x8x8x9xf32, 4294975488 : i64>) -> tensor<1x8x8x9xf32> loc(#loc11)
      %10 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 229376, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, n_idx = [0], n_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 5, stage = 1, group_type = 0>, lmem_type = 0 : i64, use_3ic_optimize = 0 : i64} : (tensor<1x8x1x1xf32, 4294979584 : i64>) -> tensor<1x8x1x1xf32> loc(#loc12)
      %11 = "tpu.Conv2D"(%8, %9, %10) {coeff_merged = false, dilations = [1, 1], do_relu = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 47616, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0, 4], n_slice = [4, 3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [78], w_idx = [0], w_slice = [38], id = 6, stage = 1, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, strides = [1, 1], use_3ic_optimize = 0 : i64, with_bias = true} : (tensor<7x8x78x38xf32>, tensor<1x8x8x9xf32>, tensor<1x8x1x1xf32>) -> tensor<7x8x78x38xf32> loc(#loc13)
      %12 = "tpu.Store"(%8) {ginfo = #tpu.lg<out_addr = 65536, out_size = 47616, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0, 4], n_slice = [4, 3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [78], w_idx = [0], w_slice = [38], id = 7, stage = 1, group_type = 0>} : (tensor<7x8x78x38xf32>) -> tensor<7x8x78x38xf32, 4295921664 : i64> loc(#loc6)
      %13 = "tpu.Add"(%11, %8) {do_relu = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 47616, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0, 4], n_slice = [4, 3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [78], w_idx = [0], w_slice = [38], id = 8, stage = 1, group_type = 0>, relu_limit = -1.000000e+00 : f64} : (tensor<7x8x78x38xf32>, tensor<7x8x78x38xf32>) -> tensor<7x8x78x38xf32> loc(#loc7)
      %14 = "tpu.Store"(%13) {ginfo = #tpu.lg<out_addr = 163840, out_size = 47616, buffer_addr = 0, buffer_size = 0, eu_align = true, n_idx = [0, 4], n_slice = [4, 3], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [78], w_idx = [0], w_slice = [38], id = 9, stage = 2, group_type = 0>} : (tensor<7x8x78x38xf32>) -> tensor<7x8x78x38xf32, 4295254016 : i64> loc(#loc7)
      tpu.Yield %12, %14 : tensor<7x8x78x38xf32, 4295921664 : i64>, tensor<7x8x78x38xf32, 4295254016 : i64> loc(#loc14)
    }) {dsecs = 1 : i64, flow = [-1, 3, 4, 5, 9, -2, 6, 7, 0, -3, 8, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 2 : i64, other_down_overlap_op = [], other_up_overlap_op = [], self_down_overlap_op = [], self_up_overlap_op = [], swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<7x3x80x40xf32, 4294983680 : i64>) -> (tensor<7x8x78x38xf32, 4295921664 : i64>, tensor<7x8x78x38xf32, 4295254016 : i64>) loc(#loc14)
    return %4#0, %4#1 : tensor<7x8x78x38xf32, 4295921664 : i64>, tensor<7x8x78x38xf32, 4295254016 : i64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("m1.weight")
#loc3 = loc("m1.bias")
#loc4 = loc("m2.weight")
#loc5 = loc("m2.bias")
#loc6 = loc("input.4_Relu")
#loc7 = loc("8_Add")
#loc8 = loc("load_0")
#loc9 = loc("load_m1.weight")
#loc10 = loc("load_m1.bias")
#loc11 = loc("load_m2.weight")
#loc12 = loc("load_m2.bias")
#loc13 = loc("/m2/Conv_output_0_Conv")
#loc14 = loc(fused[#loc6, #loc7])

